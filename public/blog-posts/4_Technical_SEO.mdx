Technical SEO is the process of making a website more search engine friendly, but it can also involve user experience-related tasks. No matter how great a website's content is, if pages on the site are inaccessible to search engines, they will not show up or rank in search results. Ensuring search engines can successfully crawl a website is the first step in optimising it for technical SEO. For example, an e-commerce business should update its product archive each time it adds a product so search engines can discover and index these new web pages.

## Website Architecture and Site Structure

Search engines like Google follow defined rules to discover and index pages. These rules can comprise of including files like the _sitemap.xml_, and the _robots.txt_, or indicating a canonical page.

Search engines have specific guidelines to follow when creating the structure for a website's URLs. Google uses a set of rules called _RFC 3986_, which is a guide for how URLs should be structured. These guidelines can include replacing reserved characters ("/", ":", "?", " ") that have a special meaning, using percent encoding. For example, a space should be replaced by "_%20"_. The sentence "_this is a sentence_" _s_hould be encoded as follows: "\_this%20is%20a%20sentence_".

Search engines know what webpages exist on a website thanks to a _sitemap_. The sitemap can describe videos, and other resources on a site as well as the connections between them. This file is examined by crawlers to improve the way they index a website. A sitemap not only gives useful information about these files but also lets Google know which pages and files are believed to be important for a website. For example, the date the page was last updated and any language variations.

A website owner can specify what web pages crawlers can index, by postulating them in a file called _robots.txt_, this file exists to avoid overloading a website with too many requests. A _robot.txt_ file should be located at the root of the domain, therefore, the _robots.txt_ file for the website "_www.example.com"_ is located at "_www.example.com/robots.txt_". The plain text file robots.txt adheres to the Robots Exclusion Standard. The file is comprised of one or more rules, each rule on the domain where the robots.txt file is placed restricts or permits access for crawlers to a defined path. All files are implicitly permitted for crawling unless your robots.txt file says otherwise. 

Sometimes, websites can have multiple URLs leading to the same content, for example: "_https://www.example.com_" can lead to the same information as "_https://example.com_". This can lead to problems with search engines because indexing both pages would be impractical. To choose which one is represented as the main one, canonicalization is used. Google selects the main content of each page when crawling it. If Google discovers many sites with primary material that seems to be identical or extremely similar, Google selects and designates the canonical page that is objectively the most comprehensive and beneficial for users. Duplicate pages are crawled less frequently compared to the canonical page to lessen the strain on sites. Canonicalization is affected by a few variables, including redirects, the URL's inclusion in a sitemap, and link annotations with the "_rel='canonical'"_ attribute. Using these methods, Google will be able to tell which page is the canonical one, but Google could choose a different page for a variety of reasons. That is to say, specifying the canonical URL is a suggestion and not a rule.

## Mobile Optimization and Responsiveness

In recent years, the use of mobile devices has skyrocketed, and this has led to an increase in search queries. A study conducted by Merkle proved that 63% of organic search engine visits have been executed by mobile devices. Additionally, according to Google 56% of in-store shoppers use their smartphone to shop and/or research products while in the store. This data has led search engines like Google to adopt a mobile-first indexing model, which means that Google will prioritize the mobile version of a website when indexing. To ensure a website has a mobile-friendly design, it must provide a consistent user experience (UX) across all devices. This means that on a PC, tablet, or phone, users must see the same images and text.

## Page Speed Optimization

Search engines consider page and site speed to be a key factor when evaluating websites. This is because search engines do not want to present results that irritate consumers by loading slowly. With their results, they aim to display the websites that are most appropriate and relevant.

The time it takes for a webpage to load is known as page speed. Numerous variables, such as the site's server, page file size, and image compression, affect how quickly a page loads. But there are many ways to measure page speed, the most common measurements of page speed are:

- **First Contentful Paint (FCP)** : It measures the duration between the moment the page loads to the point at which any content is displayed. · **10%**
- **Speed Index (SI)** : Measures how fast content is displayed · 10%
- **Largest Contentful Paint (LCP)** : Compares the render time of the largest image or text block to the initial loading time of the website · **25%**
- **Total Blocking Time (TBT)** : Metric that measures the time between the First Contentful Paint (FCP) and the moment when the user can interact with the website (TTI) · **30%**
- **Cumulative Layout Shift (CLS)** : Measures the largest spurt of unexpected movement during the lifetime of a webpage · **30%**

There are a few techniques to improve page load speeds:

- **Image Compression** : images normally account for most of a website's content file size, so tools like image compressors can shrink the size of a picture's file without sacrificing its quality. They help enhance website performance and loading times while also conserving bandwidth and storage space.
- **Content Delivery Networks** : they enable rapid delivery of resources such as HTML pages, JavaScript files, stylesheets, pictures, and videos. Nowadays, the bulk of online traffic; including traffic from well-known websites like Facebook, Netflix, and Amazon; is provided through content delivery networks (CDNs). The benefits of CDNs include: improving website load times, reduced bandwidth costs, increased content availability and redundancy and improved website security.

## Schema Markup and Structured Data

To comprehend the content of a website and to learn more about the web and the world at large, search engines use structured data that they discover online, such as details on people, books, or businesses that are mentioned as markups. This JSON-LD structured data fragment, for instance, describes the recipe's title, author, and other details and may be shown on a recipe page.

Search engines like Google can use markup data to enrich search results for local businesses with Google Knowledge Panels, which provide information about companies that match search queries. These can be prominently shown in search results when people look for businesses on Google Maps or Search. A user may see a carousel of companies linked to their query when they search for a certain type of company, for example: "best BCN restaurants". Business owners may provide Google with information about various business divisions, business hours, reviews, and more by using Local Business structured data. Maps Booking API can be used to allow bookings, payments, and other activities if a website owner wants to make it easier for users to make reservations or put orders right within search results.

## Security

Google also relies on cyber security as an SEO practice since they value user privacy. Numerous sensitive pieces of information, including customer information, login credentials, and confidential corporate information, are stored on websites. This data may be compromised by a hacker cyberattack, which might have dire repercussions including identity theft or financial loss.

Websites use encryption to protect from cyberattacks, more specifically they use SSL or Secure Sockets Layer. SSL is a mechanism for Internet security that relies on encryption created by Netscape in 1995 to guarantee data integrity, privacy, and authentication in Internet conversations. As a result, anyone attempting to intercept data will only be able to view a jumbled, practically unintelligible character mix.

Only websites with an SSL certificate can use SSL. An SSL certificate verifies that a person is who they claim to be, much like an identity card or badge. The public key of the website is among the most crucial details of an SSL certificate, which makes encryption and authentication possible. The user is given a public key while the web server possesses a secret private key which is used to decode data encrypted using the public key.
